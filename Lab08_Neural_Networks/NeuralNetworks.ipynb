{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3YDtSxjptXt"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "In this exercise you will learn how to implement a feedforward neural network and train it with backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TOF5ehVhptXv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import multivariate_normal\n",
        "from numpy.random import uniform\n",
        "from scipy.stats import zscore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo6F_ZTVptXw"
      },
      "source": [
        "We define two helper functions \"init_toy_data\" and \"init_model\" to create a simple data set to work on and a 2 layer neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rje5MYtptXx"
      },
      "source": [
        "First, we create toy data with categorical labels by sampling from different multivariate normal distributions for each class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "82g8WBTXptXx"
      },
      "outputs": [],
      "source": [
        "def init_toy_data(num_samples,num_features, num_classes, seed=3):\n",
        "    # num_samples: number of samples *per class*\n",
        "    # num_features: number of features (excluding bias)\n",
        "    # num_classes: number of class labels\n",
        "    # seed: random seed\n",
        "    np.random.seed(seed)\n",
        "    X=np.zeros((num_samples*num_classes, num_features))\n",
        "    y=np.zeros(num_samples*num_classes)\n",
        "    for c in range(num_classes):\n",
        "        # initialize multivariate normal distribution for this class:\n",
        "        # choose a mean for each feature\n",
        "        means = uniform(low=-10, high=10, size=num_features) # 주어진 범위에서 균등 분포를 따르는 난수를 생성\n",
        "        # choose a variance for each feature\n",
        "        var = uniform(low=1.0, high=5, size=num_features)\n",
        "        # for simplicity, all features are uncorrelated (covariance between any two features is 0)\n",
        "        cov = var * np.eye(num_features) # 공분산 행렬을 생성함. eye는 대각행렬을 생성함.\n",
        "        # draw samples from normal distribution\n",
        "        X[c*num_samples:c*num_samples+num_samples,:] = multivariate_normal(means, cov, size=num_samples)\n",
        "        # set label\n",
        "        y[c*num_samples:c*num_samples+num_samples] = c\n",
        "    return X,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Y28dcb7bptXy"
      },
      "outputs": [],
      "source": [
        "def init_model(input_size,hidden_size,num_classes, seed=3):\n",
        "    # input size: number of input features\n",
        "    # hidden_size: number of units in the hidden layer\n",
        "    # num_classes: number of class labels, i.e., number of output units\n",
        "    np.random.seed(seed)\n",
        "    model = {}\n",
        "    # initialize weight matrices and biases randomly\n",
        "    model['W1'] = uniform(low=-1, high=1, size=(input_size, hidden_size))  # 입력 - 은닉층 사이의 가중치를 초기화\n",
        "    model['b1'] = uniform(low=-1, high=1, size=hidden_size)                # 은닉층의 바이어스를 초기화\n",
        "    model['W2'] = uniform(low=-1, high=1, size=(hidden_size, num_classes)) # 은닉 - 출력 사이의 가중치를 초기화\n",
        "    model['b2'] = uniform(low=-1, high=1, size=num_classes)                # 출력층의 바이어스를 초기화\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2eip8kX7ptXz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: [[ 0.39636145  1.09468144 -0.89360845  0.91815536]\n",
            " [ 0.94419323 -0.94027869  1.22268078  1.29597409]\n",
            " [-1.41577399  1.15477931 -0.62099631  0.08323307]\n",
            " [-1.35264614 -0.13598976 -1.14221784  0.26928935]\n",
            " [ 0.9352123   0.38225626  1.419864   -1.51152157]\n",
            " [ 0.49265316 -1.55544856  0.01427781 -1.0551303 ]]\n",
            "y: [0. 0. 1. 1. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "# create toy data\n",
        "X,y= init_toy_data(2,4,3) # 2 samples per class; 4 features, 3 classes\n",
        "# Normalize data\n",
        "X = zscore(X, axis=0)\n",
        "print('X: ' + str(X)) # 3개의 클래스 (0,1,2)를 가지는 2개의 샘플이 있음. 각 샘플은 4개의 feature를 가짐.\n",
        "print('y: ' + str(y)) # 클래스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpCCDC2mptXz"
      },
      "source": [
        "We now initialise our neural net with one hidden layer consisting of $10$ units and and an output layer consisting of $3$ units. Here we expect (any number of) training samples with $4$ features. We do not apply any activation functions yet. The following figure shows a graphical representation of this neuronal net. \n",
        "\n",
        "<img src=\"nn.graphviz.png\"  width=\"30%\" height=\"30%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "99BAxa6RptX0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model: {'W1': array([[ 0.10159581,  0.41629565, -0.41819052,  0.02165521,  0.78589391,\n",
            "         0.79258618, -0.74882938, -0.58551424, -0.89706559, -0.11838031],\n",
            "       [-0.94024758, -0.08633355,  0.2982881 , -0.44302543,  0.3525098 ,\n",
            "         0.18172563, -0.95203624,  0.11770818, -0.48149511, -0.16979761],\n",
            "       [-0.43294984,  0.38627584, -0.11909256, -0.68626452,  0.08929804,\n",
            "         0.56062953, -0.38727294, -0.55608423, -0.22405748,  0.8727673 ],\n",
            "       [ 0.95199084,  0.34476735,  0.80566822,  0.69150174, -0.24401192,\n",
            "        -0.81556598,  0.30682181,  0.11568152, -0.27687047, -0.54989099]]), 'b1': array([-0.18696017, -0.0621195 , -0.46152884, -0.41641445, -0.0846272 ,\n",
            "        0.72106783,  0.17250581, -0.43302428, -0.44404499, -0.09075585]), 'W2': array([[-0.58917931, -0.59724258,  0.02807012],\n",
            "       [-0.82554126, -0.03282894, -0.27564758],\n",
            "       [ 0.41537324,  0.49349245,  0.38218584],\n",
            "       [ 0.37836083, -0.25279975,  0.33626961],\n",
            "       [-0.32030267,  0.14558774, -0.34838568],\n",
            "       [-0.1097099 , -0.87694214, -0.51464916],\n",
            "       [ 0.94320521, -0.53883159,  0.38295502],\n",
            "       [ 0.30095372,  0.44787828, -0.04982278],\n",
            "       [ 0.19332755, -0.86606115, -0.85487572],\n",
            "       [-0.60204795, -0.69627801, -0.79979131]]), 'b2': array([-0.74141227,  0.10655546, -0.62437035])}\n",
            "model['W1'].shape: (4, 10)\n",
            "model['W2'].shape: (10, 3)\n",
            "model['b1'].shape: (10,)\n",
            "model['b12'].shape: (3,)\n",
            "number of parameters: 83\n"
          ]
        }
      ],
      "source": [
        "# initialize model\n",
        "model = init_model(input_size=4, hidden_size=10, num_classes=3)\n",
        "\n",
        "print('model: ' + str(model)) \n",
        "print('model[\\'W1\\'].shape: ' + str(model['W1'].shape))\n",
        "print('model[\\'W2\\'].shape: ' + str(model['W2'].shape))\n",
        "print('model[\\'b1\\'].shape: ' + str(model['b1'].shape))\n",
        "print('model[\\'b12\\'].shape: ' + str(model['b2'].shape))\n",
        "print('number of parameters: ' + str((model['W1'].shape[0] * model['W1'].shape[1]) + \n",
        "     np.sum(model['W2'].shape[0] * model['W2'].shape[1]) + \n",
        "     np.sum(model['b1'].shape[0]) +\n",
        "     np.sum(model['b2'].shape[0] )))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- W1: 가중치와 바이어스를 가지는 딕셔너리. 각 feature마다 10개의 unit을 가져, 총 4 x 10 = 40개의 가중치(파라미터)를 가짐. (weight = $\\theta$)\n",
        "- b1: 10개의 unit을 가지는 은닉층의 바이어스를 가짐. (bias = $\\theta_0$)\n",
        "- w1: 10개의 unit을 가지는 은닉층과 3개의 클래스를 가지는 출력층 사이의 가중치를 가짐. 총 10 x 3 = 30개의 가중치(파라미터)를 가짐.\n",
        "- b2: 3개의 클래스를 가지는 출력층의 바이어스를 가짐."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojs6ScguptX1"
      },
      "source": [
        "<b>Exercise 1</b>: Implement softmax layer.\n",
        "\n",
        "Implement the softmax function given by \n",
        "\n",
        "$softmax(x_i) = \\frac{e^{x_i}}{{\\sum_{j\\in 1...J}e^{x_j}}}$, \n",
        "\n",
        "where $J$ is the total number of classes, i.e. the length of  **x** .\n",
        "\n",
        "Note: Implement the function such that it takes a matrix X of shape (N, J) as input rather than a single instance **x**; N is the number of instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "신경망은 대체로 분류(classification) 또는 회귀(regression) 문제를 다루는데 softmax는 분류에 사용된다. softmax 함수는 다중 클래스 분류모델을 만들 때 사용한다. 특정 인풋이이 여러 분류중 어떤 분류(eg. 강아지, 고양이, 사람)에 속하는 지를 확률로 예측해주고, 결과를 확률로 해석할 수 있게 변환해주는 함수로 높은 확률을 가지는 class로 분류한다. 위에서 xj는 소프트맥스 함수의 입력값이다. \n",
        "이는 (j번째 입력값) / (입력값의 합)으로 볼 수 있으며, 따라서 확률이다. 지수함수가 사용되는 이유는 미분이 가능하도록 하게 함이며, 입력값 중 큰 값은 더 크게 작은값은 더 작게 만들어 입력벡터가 더 잘 구분되도록 한다. \n",
        "\n",
        "위 예시에서, xi는 출력층의 뉴런 중 i번째를 뜻하고, j는 출력층의 뉴런 수, 즉 클래스 수를 의미한다. 간단히 말해서, 분자는 입력신호 xi의 지수함수, 분모는 모든 입력신호의 지수함수의 합이라고 볼수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GcgC5wMvptX1"
      },
      "outputs": [],
      "source": [
        "def softmax(X):\n",
        "    #print(X.shape)\n",
        "    X = X - np.max(X, axis=1, keepdims=True) # overflow 방지, normalization\n",
        "    #print(X)\n",
        "    exi = np.exp(X)\n",
        "    exj = np.sum(exi, axis=1, keepdims=True) # 각 행별로 합\n",
        "    xi = np.zeros((len(X), len(X[0])))\n",
        "    for i in range(len(X)):\n",
        "        xi[i] = exi[i] / exj[i]\n",
        "        \n",
        "    return xi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-BcVCIqptX2"
      },
      "source": [
        "Check if everything is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "recCBdmqptX2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing successful.\n",
            "[[0.35434369 0.64565631]\n",
            " [0.57444252 0.42555748]]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([[0.1, 0.7],[0.7,0.4]])\n",
        "exact_softmax = np.array([[ 0.35434369,  0.64565631],\n",
        "                         [ 0.57444252,  0.42555748]])\n",
        "sm = softmax(x)\n",
        "difference = np.sum(np.abs(exact_softmax - sm))\n",
        "try:\n",
        "    assert difference < 0.000001   \n",
        "    print(\"Testing successful.\")\n",
        "    print(sm)\n",
        "except:\n",
        "    print(\"Tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO0gCmA3ptX3"
      },
      "source": [
        "<b>Exercise 2</b>: Implement the forward propagation algorithm for the model defined above.\n",
        "\n",
        "The activation function of the hidden neurons is a Rectified Linear Unit $relu(x)=max(0,x)$ (to be applied element-wise to the hidden units)\n",
        "The activation function of the output layer is a softmax function as (as implemented in Exercise 1).\n",
        "\n",
        "The function should return both the activation of the hidden units (after having applied the $relu$ activation function) (shape: $(N, num\\_hidden)$) and the softmax model output (shape: $(N, num\\_classes)$). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pze-k4-XptX3"
      },
      "outputs": [],
      "source": [
        "def forward_prop(X,model):\n",
        "    W1=model['W1']\n",
        "    b1=model['b1']\n",
        "    W2=model['W2']\n",
        "    b2=model['b2']\n",
        "\n",
        "    # activate potential h = weight * x0 + bias\n",
        "    H = np.dot(X, W1) + b1\n",
        "    # x1 = activation(h)\n",
        "    hidden_activations = np.maximum(H, 0) # using ReLU as a activation\n",
        "    # output potential o = weight * x1 + bias\n",
        "    O = np.dot(hidden_activations, W2) + b2\n",
        "    # output = softmax(o)\n",
        "    probs = softmax(O)\n",
        "    \n",
        "    return hidden_activations,probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SHrLsiylptX3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing successful.\n"
          ]
        }
      ],
      "source": [
        "acts,probs = forward_prop(X, model)\n",
        "correct_probs = np.array([[0.22836388, 0.51816433, 0.25347179],\n",
        "                            [0.15853289, 0.33057078, 0.51089632],\n",
        "                            [0.40710319, 0.41765056, 0.17524624],\n",
        "                            [0.85151353, 0.03656425, 0.11192222],\n",
        "                            [0.66016592, 0.19839791, 0.14143618],\n",
        "                            [0.70362036, 0.08667923, 0.20970041]])\n",
        "\n",
        "# the difference should be very small.\n",
        "difference =  np.sum(np.abs(probs - correct_probs))\n",
        "\n",
        "try:\n",
        "    assert probs.shape==(X.shape[0],len(set(y)))\n",
        "    assert difference < 0.00001   \n",
        "    print(\"Testing successful.\")\n",
        "except:\n",
        "    print(\"Tests failed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGef2WaLptX4"
      },
      "source": [
        "<b>Exercise 3:</b> \n",
        "\n",
        "How would you train the above defined neural network? Which loss-function would you use? You do not need to implement this.\n",
        "\n",
        "- for training through back propagation, we need optimizer and evaluation metric. And with these, we have to complie the model.\n",
        "- we used softmax as a function for output layer, so we can use softmax-loss for calculating loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSP8x8NEptX4"
      },
      "source": [
        "<b>Part 2 (Neural Net using Keras)</b>\n",
        "\n",
        "Instead of implementing the model learning ourselves, we can use the neural network library Keras for Python (https://keras.io/). Keras is an abstraction layer that either builds on top of Theano or Google's Tensorflow. So please install Keras and Tensorflow/Theano for this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKozKPKmptX4"
      },
      "source": [
        "<b>Exercise 4:</b>\n",
        "    Implement the same model as above using Keras:\n",
        "    \n",
        "    ** 1 hidden layer à 10 units\n",
        "    ** softmax output layer à three units\n",
        "    ** 4 input features\n",
        "    \n",
        "Compile the model using categorical cross-entropy (also referred to as 'softmax-loss') as loss function and using categorical crossentropy together with categorical accuracy as metrics for runtime evaluation during training.\n",
        "\n",
        "Hint 1: Use the Sequential Class API of Keras (https://keras.io/api/models/sequential/ or https://www.tensorflow.org/guide/keras/sequential_model)\n",
        "\n",
        "Hint 2: You can use the Adam optimizer of Keras for the model compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pZqK7kxhptX5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83</span> (332.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m83\u001b[0m (332.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83</span> (332.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83\u001b[0m (332.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# define the model \n",
        "model = Sequential() # from tensorflow.keras.models\n",
        "model.add(Dense(10, input_dim=4, activation=\"relu\")) # 10 hidden units, 4 input units\n",
        "#model.add(Activation(\"relu\"))\n",
        "model.add(Dense(3, activation=\"softmax\")) # 3 output units\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"categorical_accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QhNMmKGptX5"
      },
      "source": [
        "The description of the current network can always be looked at via the summary method. The layers can be accessed via model.layers and weights can be obtained with the method get_weights. Check if your model is as expected. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "abzHV5AxptX5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First layer weights: [[-0.3349277  -0.20815682  0.07303619  0.5585607   0.43144965 -0.3591347\n",
            "  -0.14473945 -0.46396878  0.4558953  -0.26218727]\n",
            " [ 0.02293992  0.5296025   0.47306192 -0.30638427  0.479069    0.5002371\n",
            "  -0.2770795  -0.4525519   0.2796756   0.64514565]\n",
            " [ 0.5460888   0.16778523  0.3889724  -0.32512313  0.1853447  -0.20481151\n",
            "  -0.4875121   0.20600724 -0.53105044  0.31995165]\n",
            " [ 0.21698058 -0.1970346   0.63833404  0.20191354  0.2528442  -0.00997531\n",
            "   0.6020616   0.6002233  -0.64970714  0.6070931 ]]; shape: (4, 10)\n",
            "First layer bias: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]; shape: (10,)\n",
            "Second layer weights: [[-5.2669907e-01  3.1668967e-01 -1.9972163e-01]\n",
            " [ 7.0066333e-02  2.0276010e-02 -6.4692861e-01]\n",
            " [ 4.4379890e-01 -3.5927296e-03 -4.0056151e-01]\n",
            " [ 2.9915571e-04 -3.7570083e-01  2.8342664e-01]\n",
            " [-6.1577672e-01 -3.0768493e-01 -4.1390425e-01]\n",
            " [-6.4112395e-01 -3.2214803e-01 -6.5467513e-01]\n",
            " [-1.4445269e-01 -6.3309640e-01  5.1082373e-01]\n",
            " [-4.6026725e-01 -1.7106354e-02  5.6230903e-02]\n",
            " [-7.8847885e-02 -5.7944280e-01  3.3634353e-01]\n",
            " [ 2.7543974e-01 -9.8765969e-02  2.8422838e-01]]; shape: (10, 3)\n",
            "Second layer bias: [0. 0. 0.]; shape: (3,)\n",
            "number of layes: 2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m50\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83</span> (332.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m83\u001b[0m (332.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">83</span> (332.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m83\u001b[0m (332.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check model architecture and initial weights.\n",
        "\n",
        "W_1, b_1 = model.layers[0].get_weights()\n",
        "print(\"First layer weights: %s; shape: %s\" % (W_1,W_1.shape))\n",
        "print(\"First layer bias: %s; shape: %s\" % (b_1,b_1.shape))\n",
        "W_2, b_2 = model.layers[1].get_weights()\n",
        "print(\"Second layer weights: %s; shape: %s\" % (W_2,W_2.shape))\n",
        "print(\"Second layer bias: %s; shape: %s\" % (b_2,b_2.shape))\n",
        "print(\"number of layes: \" + str(len(model.layers)))\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "sk8x5Dy0ptX5"
      },
      "source": [
        "<b>Exercise 5:</b> Train the model on the toy data set generated below: \n",
        "\n",
        "Hints: \n",
        "\n",
        "* Keras expects one-hot-coded labels \n",
        "\n",
        "* Don't forget to normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_4mu3twRptX6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - categorical_accuracy: 0.7069 - loss: 0.8321 - val_categorical_accuracy: 0.8101 - val_loss: 0.6600\n",
            "Epoch 2/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - categorical_accuracy: 0.8054 - loss: 0.6229 - val_categorical_accuracy: 0.8788 - val_loss: 0.4965\n",
            "Epoch 3/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - categorical_accuracy: 0.8910 - loss: 0.4621 - val_categorical_accuracy: 0.9313 - val_loss: 0.3792\n",
            "Epoch 4/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - categorical_accuracy: 0.9333 - loss: 0.3577 - val_categorical_accuracy: 0.9576 - val_loss: 0.2909\n",
            "Epoch 5/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - categorical_accuracy: 0.9631 - loss: 0.2716 - val_categorical_accuracy: 0.9687 - val_loss: 0.2239\n",
            "Epoch 6/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - categorical_accuracy: 0.9774 - loss: 0.2173 - val_categorical_accuracy: 0.9778 - val_loss: 0.1735\n",
            "Epoch 7/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - categorical_accuracy: 0.9885 - loss: 0.1595 - val_categorical_accuracy: 0.9838 - val_loss: 0.1368\n",
            "Epoch 8/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - categorical_accuracy: 0.9861 - loss: 0.1310 - val_categorical_accuracy: 0.9889 - val_loss: 0.1090\n",
            "Epoch 9/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - categorical_accuracy: 0.9924 - loss: 0.1015 - val_categorical_accuracy: 0.9919 - val_loss: 0.0877\n",
            "Epoch 10/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952us/step - categorical_accuracy: 0.9958 - loss: 0.0832 - val_categorical_accuracy: 0.9939 - val_loss: 0.0708\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432us/step - categorical_accuracy: 0.9942 - loss: 0.0731\n",
            "Train Accuracy: 0.9960\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - categorical_accuracy: 0.9933 - loss: 0.0690\n",
            "Test Accuracy: 0.9939\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = init_toy_data(1000,4,3, seed=3)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=67)\n",
        "\n",
        "# Normalize data\n",
        "X_train = zscore(X_train, axis=0)\n",
        "X_test = zscore(X_test, axis=0)\n",
        "\n",
        "# one-hot encoding\n",
        "y_train = to_categorical(y_train, num_classes=3)\n",
        "y_test = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "# train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)) # epochs and batch_size, refer to the example provided\n",
        "\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np35zqMPjrjo"
      },
      "source": [
        "Compare the test accuracy with the train accuracy. What can you see? Is the model performing well?\n",
        "\n",
        "- You can see that accuracy gradually increases with each epoch, and loss gradually decreases.\n",
        "- The accuracy of both the train set and the test set are high, and they match well. The model seems to be working well."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
