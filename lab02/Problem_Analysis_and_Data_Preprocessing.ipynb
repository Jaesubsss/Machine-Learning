{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WhpMgiS1o5A"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "The goal of this lab is to introduce you to data preprocessing techniques in order to make your data suitable for applying a learning algorithm.\n",
        "\n",
        "## 1. Handling Missing Values\n",
        "\n",
        "A common (and very unfortunate) data property is the ocurrence of missing and erroneous values in multiple features in datasets. For this exercise we will be using a data set about abalone snails.\n",
        "The data set is contained in the Zip file you downloaded from Moodle (abalone.csv).\n",
        "\n",
        "To determine the age of a abalone snail you have to kill the snail and count the annual\n",
        "rings. You are told to estimate the age of a snail on the basis of the following attributes:\n",
        "1. type: male (0), female (1) and infant (2)\n",
        "2. length in mm\n",
        "3. width in mm\n",
        "4. height in mm\n",
        "5. total weight in grams\n",
        "6. weight of the meat in grams\n",
        "7. drained weight in grams\n",
        "8. weight of the shell in grams\n",
        "9. number of annual rings (number of rings +1, 5 yields age)\n",
        "\n",
        "However, the data is incomplete. Missing values are marked with −1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aTRoZnye1o5D",
        "outputId": "3b2669b0-1ea1-46d6-b768-638722442986",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.3515</td>\n",
              "      <td>0.1410</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.120</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
              "0     0   0.350  0.265   0.090        0.2255       0.0995          0.0485   \n",
              "1     1   0.530  0.420   0.135        0.6770       0.2565          0.1415   \n",
              "2     0   0.440  0.365   0.125        0.5160       0.2155          0.1140   \n",
              "3     2  -1.000  0.255   0.080        0.2050       0.0895          0.0395   \n",
              "4     2   0.425  0.300   0.095        0.3515       0.1410          0.0775   \n",
              "\n",
              "   shell_weight  num_rings  \n",
              "0         0.070         -1  \n",
              "1         0.210          9  \n",
              "2         0.155         10  \n",
              "3         0.055          7  \n",
              "4         0.120          8  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# load data \n",
        "df = pd.read_csv(\"http://www.cs.uni-potsdam.de/ml/teaching/ss15/ida/uebung02/abalone.csv\") #Should this not work please use the csv that was part of the zip file.\n",
        "df.columns=['type','length','width','height','total_weight','meat_weight','drained_weight','shell_weight','num_rings']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_KMa47j1o5E"
      },
      "source": [
        "### Exercise 1.1\n",
        "\n",
        "Compute the mean of of each numeric column and the counts of each categorical column, excluding the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dCq2-NW1o5F"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.91</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.21</td>\n",
              "      <td>9.66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
              "0  0.91    0.48   0.37    0.11          0.78         0.32            0.15   \n",
              "\n",
              "   shell_weight  num_rings  \n",
              "0          0.21       9.66  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 결측치를 제외하고 평균을 추출하라\n",
        "\n",
        "#df.replace(-1, pd.NA, inplace=True)  # -1을 NA로 변경, inplace=True로 원본을 변경\n",
        "\n",
        "df_nomv = df.replace(-1, pd.NA) # missing value -1 to NA, to exclude missing value out of mean calculation\n",
        "\n",
        "df_mean = [df[colname].mean() for colname in df.columns] # calculate mean of each column\n",
        "\n",
        "#df_nomv.head()\n",
        "df_mean = pd.DataFrame([df_mean], columns=df.columns) # convert data in list to dataframe\n",
        "\n",
        "df_mean.head()\n",
        "df_mean.round(2) # decimal point 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0CjV2c1o5G"
      },
      "source": [
        "### Exercise 1.2\n",
        "\n",
        "Compute the median of each numeric column,  excluding the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sw_28SAt1o5G"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.22</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
              "0   1.0    0.54   0.42    0.14          0.78         0.33            0.17   \n",
              "\n",
              "   shell_weight  num_rings  \n",
              "0          0.22        9.0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 결측치를 제외하고 중앙값을 추출하라\n",
        "\n",
        "df_median = [df[colname].median() for colname in df.columns] # calculate median of each column\n",
        "df_median = pd.DataFrame([df_median], columns=df.columns) # convert data in list to dataframe\n",
        "\n",
        "df_median.head()\n",
        "df_median.round(2) # decimal point 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMltOlp_1o5G"
      },
      "source": [
        "### Exercise 1.3\n",
        "\n",
        "Handle the missing values in a way that you find suitable. Think about different ways. Discuss dis-/advantages of your approach. Argue your choices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Missing values를 처리하는 많은 방법이 있다. 각 방법의 장단점을 고려하여 적절한 방법을 찾아보도록 하자.\n",
        "\n",
        "개수가 적은 경우에는 인스턴스 자체를 삭제해버릴 수도 있다. 아니면 새로운 binary attribute를 넣어서 결측 유무를 표시할수도 있다.\n",
        "\n",
        "\n",
        "평균 또는 중간값으로 해당 값을 대체할수도 있고, Regression이나 예측 모델을 사용할수도 있다.\n",
        "\n",
        "해당 데이터를 삭제해버리는 것은 너무 brutal하니, numerical 밸류는 평균으로, categorical은 median으로 대체해보도록 하자\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.07</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.677</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.21</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>&lt;NA&gt;</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.3515</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.12</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  type length  width height total_weight meat_weight drained_weight  \\\n",
              "0    0   0.35  0.265   0.09       0.2255      0.0995         0.0485   \n",
              "1    1   0.53   0.42  0.135        0.677      0.2565         0.1415   \n",
              "2    0   0.44  0.365  0.125        0.516      0.2155          0.114   \n",
              "3    2   <NA>  0.255   0.08        0.205      0.0895         0.0395   \n",
              "4    2  0.425    0.3  0.095       0.3515       0.141         0.0775   \n",
              "\n",
              "  shell_weight num_rings  \n",
              "0         0.07      <NA>  \n",
              "1         0.21         9  \n",
              "2        0.155        10  \n",
              "3        0.055         7  \n",
              "4         0.12         8  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_nomv.head()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gxDCHrb31o5G"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\exima\\AppData\\Local\\Temp\\ipykernel_15940\\438668909.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_new['type'] = df_new['type'].replace(pd.NA, df_median['type'][0]) # replace missing value in 'type' column with median value\n",
            "C:\\Users\\exima\\AppData\\Local\\Temp\\ipykernel_15940\\438668909.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_new['num_rings'] = df_new['num_rings'].replace(pd.NA, df_median['num_rings'][0])# same for ring column\n",
            "C:\\Users\\exima\\AppData\\Local\\Temp\\ipykernel_15940\\438668909.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_new[col] = df_new[col].replace(pd.NA, df_mean[col][0])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.07</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.21</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.16</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.06</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4171</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.25</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4172</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.26</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4173</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.31</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4174</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.30</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4175</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.95</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.50</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4176 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
              "0      0.0    0.35   0.26    0.09          0.23         0.10            0.05   \n",
              "1      1.0    0.53   0.42    0.14          0.68         0.26            0.14   \n",
              "2      0.0    0.44   0.36    0.12          0.52         0.22            0.11   \n",
              "3      2.0    0.48   0.26    0.08          0.20         0.09            0.04   \n",
              "4      2.0    0.42   0.30    0.10          0.35         0.14            0.08   \n",
              "...    ...     ...    ...     ...           ...          ...             ...   \n",
              "4171   1.0    0.56   0.45    0.16          0.89         0.37            0.24   \n",
              "4172   0.0    0.59   0.44    0.14          0.97         0.44            0.21   \n",
              "4173   0.0    0.60   0.48    0.20          1.18         0.53            0.29   \n",
              "4174   1.0    0.62   0.48    0.15          0.78         0.53            0.26   \n",
              "4175   0.0    0.71   0.56    0.20          1.95         0.95            0.38   \n",
              "\n",
              "      shell_weight  num_rings  \n",
              "0             0.07        9.0  \n",
              "1             0.21        9.0  \n",
              "2             0.16       10.0  \n",
              "3             0.06        7.0  \n",
              "4             0.12        8.0  \n",
              "...            ...        ...  \n",
              "4171          0.25       11.0  \n",
              "4172          0.26       10.0  \n",
              "4173          0.31        9.0  \n",
              "4174          0.30       10.0  \n",
              "4175          0.50       12.0  \n",
              "\n",
              "[4176 rows x 9 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_new = df_nomv는 df_nomv의 주소를 참조하므로 df_nomv를 변경하면 df_new도 변경된다.\n",
        "# df_new = df_nomv is reference of df_nomv, so if df_nomv is changed, df_new is also changed.\n",
        "df_new  = df_nomv.copy() # copy dataframe to new dataframe\n",
        "\n",
        "df_new['type'] = df_new['type'].replace(pd.NA, df_median['type'][0]) # replace missing value in 'type' column with median value\n",
        "\n",
        "df_new['num_rings'] = df_new['num_rings'].replace(pd.NA, df_median['num_rings'][0])# same for ring column\n",
        "#df_new['num_rings'] = df_new['num_rings'].fillna(df_median['num_rings'][0])  # from chatGPT, it's supposed to execute same as above\n",
        "\n",
        "# fill missing value with mean value from corresponding column\n",
        "for col in df_new.columns:\n",
        "    df_new[col] = df_new[col].replace(pd.NA, df_mean[col][0])\n",
        "\n",
        "df_new.head()\n",
        "df_new.round(2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpklBouL1o5H"
      },
      "source": [
        "### Exercise 1.4\n",
        "\n",
        "Perform Z-score normalization on every column (except the type of course!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HbkY--hk1o5I"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>total_weight</th>\n",
              "      <th>meat_weight</th>\n",
              "      <th>drained_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>num_rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.17</td>\n",
              "      <td>-4.05</td>\n",
              "      <td>-3.88</td>\n",
              "      <td>-3.25</td>\n",
              "      <td>-1.49</td>\n",
              "      <td>-1.53</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>5.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.17</td>\n",
              "      <td>-3.87</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-3.20</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>-1.38</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>-1.52</td>\n",
              "      <td>5.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.17</td>\n",
              "      <td>-3.96</td>\n",
              "      <td>-3.78</td>\n",
              "      <td>-3.21</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>-1.57</td>\n",
              "      <td>6.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.83</td>\n",
              "      <td>-3.92</td>\n",
              "      <td>-3.89</td>\n",
              "      <td>-3.26</td>\n",
              "      <td>-1.51</td>\n",
              "      <td>-1.54</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>-1.67</td>\n",
              "      <td>3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.83</td>\n",
              "      <td>-3.97</td>\n",
              "      <td>-3.85</td>\n",
              "      <td>-3.24</td>\n",
              "      <td>-1.36</td>\n",
              "      <td>-1.49</td>\n",
              "      <td>-1.58</td>\n",
              "      <td>-1.61</td>\n",
              "      <td>4.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type  length  width  height  total_weight  meat_weight  drained_weight  \\\n",
              "0 -1.17   -4.05  -3.88   -3.25         -1.49        -1.53           -1.61   \n",
              "1 -0.17   -3.87  -3.73   -3.20         -1.03        -1.38           -1.52   \n",
              "2 -1.17   -3.96  -3.78   -3.21         -1.20        -1.42           -1.55   \n",
              "3  0.83   -3.92  -3.89   -3.26         -1.51        -1.54           -1.62   \n",
              "4  0.83   -3.97  -3.85   -3.24         -1.36        -1.49           -1.58   \n",
              "\n",
              "   shell_weight  num_rings  \n",
              "0         -1.66        5.9  \n",
              "1         -1.52        5.9  \n",
              "2         -1.57        6.9  \n",
              "3         -1.67        3.9  \n",
              "4         -1.61        4.9  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z_df = df_new - df_new.mean() / df_new.std() # z-score normalization\n",
        "z_df = z_df.round(2)\n",
        "z_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krOpdi_i1o5J"
      },
      "source": [
        "## 2. Preprocessing text (Optional)\n",
        "\n",
        "One possible way to transform text documents into vectors of numeric attributes is to use the TF-IDF representation. We will experiment with this representation using the 20 Newsgroup data set. The data set contains postings on 20 different topics. The classification problem is to decide which of the topics a posting falls into. Here, we will only consider postings about medicine and space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TmhZ8_FC1o5J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The index of each category is: [(0, 'sci.med'), (1, 'sci.space')]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "\n",
        "categories = ['sci.med', 'sci.space'] # train은 트레이닝 데이터 서브셋\n",
        "raw_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42) # random_state는 난수 생성할때 쓰이는 시드값. 같은 시드값을 주면 같은 난수가 생성된다.\n",
        "# raw_data는 dictionary 형태로 저장되어 있음. keys()로 확인 가능.\n",
        "print(f'The index of each category is: {[(i,target) for i,target in enumerate(raw_data.target_names)]}') # target_names는 카테고리 이름을 저장하고 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kWdpZz61o5K"
      },
      "source": [
        "Check out some of the postings, might find some funny ones!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CFZgvye31o5K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "943\n",
            "This is a sci.med email.\n",
            "\n",
            "There are 1187 emails.\n",
            "\n",
            "From: geb@cs.pitt.edu (Gordon Banks)\n",
            "Subject: Re: Could this be a migraine?\n",
            "Reply-To: geb@cs.pitt.edu (Gordon Banks)\n",
            "Organization: Univ. of Pittsburgh Computer Science\n",
            "Lines: 34\n",
            "\n",
            "\n",
            "In article <20773.3049.uupcb@factory.com> jim.zisfein@factory.com (Jim Zisfein) writes:\n",
            "\n",
            ">Headaches that seriously interfere with activities of daily living\n",
            ">affect about 15% of the population.  Doesn't that sound like\n",
            ">something a \"primary care\" physician should know something about?  I\n",
            ">tend to agree with HMO administrators - family physicians should\n",
            ">learn the basics of headache management.\n",
            ">\n",
            "Absolutely.  Unfortunately, most of them have had 3 weeks of neurology\n",
            "in medical school and 1 month (maybe) in their residency.  Most\n",
            "of that is done in the hospital where migraines rarely are seen.\n",
            "Where are they supposed to learn?  Those who are diligent and\n",
            "read do learn, but most don't, unfortunately.\n",
            "\n",
            ">Sometimes I wonder what tension-type headaches have to do with\n",
            ">neurology anyway.\n",
            "\n",
            "We are the only ones, sometimes, who have enough interest in headaches\n",
            "to spend the time to get enough history to diagnose them.  Too often,\n",
            "the primary care physician hears \"headache\" and loses interest in\n",
            "anything but giving the patient analgesics and getting them out of\n",
            "the office so they can get on to something more interesting.\n",
            "\n",
            "\n",
            ">(I am excepting migraine, which is arguably neurologic).  Headaches\n",
            "\n",
            "I hope you meant \"inarguably\".\n",
            "\n",
            "-- \n",
            "----------------------------------------------------------------------------\n",
            "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the intellect, and\n",
            "geb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon.\" \n",
            "----------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "idx = np.random.randint(0, len(raw_data.data)) #\n",
        "print(raw_data.target[idx]) # target은 카테고리 인덱스를 나타냄. idx는 랜덤으로 추출한 이메일의 인덱스. 즉 그 이메일이 어떤 카테고리에 속하는지 나타냄.\n",
        "print(idx)\n",
        "print (f'This is a {raw_data.target_names[raw_data.target[idx]]} email.\\n') #raw_data.target[idx]는 해당 이메일의 카테고리 인덱스를 나타냄.\n",
        "print (f'There are {len(raw_data.data)} emails.\\n') # 전체 이메일 수\n",
        "print(raw_data.data[idx]) # 전체 이메일 중 idx번째 이메일을 출력."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytNRgBtD1o5L"
      },
      "source": [
        "Lets pick the first 10 postings from each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7XjYd0ML1o5L"
      },
      "outputs": [],
      "source": [
        "idxs_med = np.flatnonzero(raw_data.target == 0) # 각 카테고리에 맞는 해당 변수의 인덱스를 추출해서 새 변수에 저장\n",
        "# print(idxs_med)\n",
        "idxs_space = np.flatnonzero(raw_data.target == 1)\n",
        "# flatnonzero 함수는 0이 아닌 요소의 인덱스를 반환하는 함수. 여기서는 target이 0인 인덱스를 추출.\n",
        "# 이 함수는 다차원 배열을 1차원으로 평평하게 만든 후, 0이 아닌 요소의 인덱스를 반환.\n",
        "\n",
        "idxs = np.concatenate([idxs_med[:10],idxs_space[:10]]) # 각 카테고리에서 맨 앞 10개의 인덱스를 추출해서 idx에 저장. \n",
        "# concatenate 함수는 두 배열을 합치는 함수. 여기서는 0~9번째까지의 med와 0~9번째까지의 space를 합쳐서 20개를 추출.\n",
        "\n",
        "data = np.array(raw_data.data) # 인덱스에 해당하는 데이터를 추출하기 위해 numpy array로 변환\n",
        "data = data[idxs] # 인덱스에 해당하는 데이터만 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY4YffVy1o5M"
      },
      "source": [
        "<a href=\"http://www.nltk.org/\">NLTK</a> is a toolkit for natural language processing. Take some time to install it and go through this <a href=\"http://www.slideshare.net/japerk/nltk-in-20-minutes\">short tutorial/presentation</a>. (or use e.g. Google colab where the package is prepared already)\n",
        "\n",
        "The downloaded package below is a tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GhpnijnB1o5M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\exima\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import itertools\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the sentences into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in data]\n",
        "vocabulary_size = 1000\n",
        "unknown_token = 'unknown'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "토큰화(tokenization)는 텍스트를 작은 단위로 나누는 과정을 말합니다. 이 작은 단위는 주로 단어, 문장 또는 문단이 될 수 있습니다. 일반적으로 자연어 처리에서는 문장을 단어로 분할하는 것이 일반적입니다. 예를 들어, \"Hello, how are you?\"라는 문장을 토큰화하면 다음과 같이 단어로 분할됩니다:\n",
        "\n",
        "원본 문장: \"Hello, how are you?\"\n",
        "\n",
        "토큰화 결과: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"]\n",
        "\n",
        "이렇게 텍스트를 단어 단위로 나누는 과정을 토큰화라고 합니다. 토큰화는 자연어 처리 작업에서 매우 중요한 전처리 단계 중 하나입니다. 이를 통해 텍스트 데이터를 모델에 입력으로 사용할 수 있도록 준비할 수 있습니다.\n",
        "\n",
        "위 코드는 NLTK에서 제공하는 문장 토크나이저인 punkt를 다운로드하고, 이를 사용하여 문장을 단어로 토큰화하는 작업을 수행합니다.\n",
        "\n",
        "1. `nltk.download('punkt')`: NLTK에서 제공하는 punkt 토크나이저를 다운로드하는 명령입니다. 이 토크나이저는 문장을 단어로 토큰화하는 데 사용됩니다.\n",
        "2. `tokenized_sentences` = [nltk.word_tokenize(sent) for sent in data]: 각 문장을 단어로 토큰화하여 리스트에 저장합니다. data는 토큰화할 문장의 리스트입니다.\n",
        "3. `vocabulary_size = 1000`: 단어 집합의 크기를 지정합니다. 이는 모델이 학습할 최대 단어의 개수를 결정합니다.\n",
        "4. `unknown_token = 'unknown'`: 알려지지 않은 단어(unknown word)를 나타내는 특별한 토큰을 정의합니다. 모델이 학습되지 않은 단어는 이 토큰으로 대체됩니다.\n",
        "\n",
        "이제 tokenized_sentences에는 각 문장이 단어로 토큰화된 리스트가 저장되어 있습니다. 이를 사용하여 자연어 처리 작업을 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LvKCOBjx1o5M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1636 unique words tokens.\n"
          ]
        }
      ],
      "source": [
        "# Count the word frequencies\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print (f\"Found {len(word_freq.items())} unique words tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위 코드는 NLTK의 `FreqDist`를 사용하여 토큰화된 문장에서 고유한 단어 토큰의 빈도를 계산하는 작업을 수행합니다.\n",
        "\n",
        "- `itertools.chain(*tokenized_sentences)`는 `tokenized_sentences` 리스트에 있는 각 문장의 단어 토큰 리스트를 하나의 이터레이터로 결합합니다. 이는 여러 리스트를 하나의 단일 리스트로 만듭니다.\n",
        "\n",
        "- `nltk.FreqDist()`는 주어진 리스트의 요소의 빈도를 계산합니다. 여기서는 모든 문장의 단어 토큰을 하나의 리스트로 결합한 이터레이터를 입력으로 사용합니다.\n",
        "\n",
        "- `word_freq.items()`는 빈도 분포 객체에서 각 단어와 해당 빈도를 포함하는 (단어, 빈도) 튜플의 리스트를 반환합니다. 이 리스트의 길이를 계산하여 총 고유한 단어 토큰의 수를 출력합니다.\n",
        "\n",
        "여기서 '이터레이터(iterator)'란 파이썬에서 반복 가능한(iterable) 객체에서 원소를 하나씩 순서대로 꺼내올 수 있는 객체를 말합니다. 이터레이터는 주로 for 루프를 사용하여 순회하거나 next() 함수를 사용하여 원소를 하나씩 가져오는 데 사용됩니다.\n",
        "\n",
        "예를 들어, 리스트, 튜플, 세트 등의 컬렉션은 이터레이터입니다. 즉, 이들은 반복 가능하며 for 루프를 사용하여 순회할 수 있습니다. 하지만 이터레이터는 전체 요소를 한 번에 메모리에 저장하지 않고 필요한 만큼 요소를 생성하므로 메모리를 효율적으로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yw_h_8Vo1o5N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using vocabulary size 1000.\n",
            "The least frequent word in our vocabulary is 'AN' and appeared 1 times.\n",
            "index to word is e\n",
            "152\n",
            "[(':', 158), ('.', 152), (',', 142), ('--', 139), ('>', 136), ('the', 124), (')', 88), ('to', 86), ('(', 83), ('of', 74), ('@', 73), ('a', 63), ('and', 63), ('I', 58), ('that', 54), ('is', 53), ('in', 42), ('it', 37), ('be', 36), ('?', 36), ('for', 35), ('!', 32), ('this', 31), (\"n't\", 30), ('*', 26), ('are', 24), (\"'s\", 23), ('From', 21), ('do', 21), ('Subject', 20), ('Organization', 20), ('Lines', 20), (\"''\", 20), ('on', 19), ('have', 18), ('as', 17), ('not', 17), ('``', 16), ('you', 16), ('In', 15), ('an', 15), ('was', 15), ('we', 14), ('Re', 13), ('The', 13), ('-', 13), ('<', 12), ('would', 12), ('if', 12), ('o', 12), ('writes', 11), ('will', 11), ('It', 11), ('but', 11), ('or', 11), ('they', 11), ('Space', 11), ('article', 10), ('may', 10), ('with', 10), ('food', 10), ('by', 10), ('what', 10), ('...', 10), ('see', 10), ('like', 9), ('should', 9), ('can', 9), ('there', 9), ('some', 9), ('about', 9), ('at', 9), ('know', 8), ('up', 8), ('who', 8), ('..', 8), ('Griffin', 8), ('out', 7), ('which', 7), ('Is', 7), ('diet', 7), ('one', 7), ('inflammation', 7), ('used', 7), ('stage', 7), ('any', 6), ('does', 6), ('Crohn', 6), ('has', 6), ('things', 6), ('But', 6), ('my', 6), ('them', 6), ('more', 6), ('3', 6), ('all', 6), ('space', 6), ('$', 6), ('billion', 6), ('Reply-To', 5), ('Ken', 5), ('anyone', 5), ('their', 5), (\"'d\", 5), ('cause', 5), ('problems', 5), ('Steve', 5), ('because', 5), ('been', 5), ('told', 5), ('doctor', 5), ('now', 5), ('had', 5), ('good', 5), ('anything', 5), ('point', 5), ('when', 5), ('think', 5), ('just', 5), ('from', 5), ('people', 5), ('[', 5), (']', 5), ('University', 5), ('body', 5), ('high', 5), ('so', 5), ('MSG', 5), ('very', 5), ('wife', 5), ('THE', 5), ('FOOD', 5), ('SOME', 5), ('1', 5), ('dougb', 5), ('Doug', 5), ('need', 5), (\"'m\", 5), (\"'\", 5), ('Office', 5), ('Does', 4), ('called', 4), ('without', 4), ('Robert', 4), ('My', 4), (\"'ve\", 4), ('other', 4), ('David', 4), ('For', 4), ('A', 4), ('person', 4), ('no', 4), ('simply', 4), ('There', 4), ('patients', 4), ('causes', 4), ('get', 4), ('try', 4), ('getting', 4), ('were', 4), ('only', 4), ('few', 4), ('If', 4), ('while', 4), ('able', 4), ('than', 4), ('could', 4), ('5', 4), ('concerned', 4), ('around', 4), ('Nntp-Posting-Host', 4), ('IN', 4), ('IS', 4), ('OF', 4), ('4', 4), ('read', 4), ('ultrasound', 4), ('radiologist', 4), ('where', 4), ('nsmca', 4), ('acad3.alaska.edu', 4), ('station', 4), ('least', 4), ('into', 4), ('he', 4), ('Mars', 4), ('first', 4), ('kjenks', 4), ('gothamcity.jsc.nasa.gov', 4), ('NASA', 4), ('szabo', 4), ('techbook.com', 4), ('version', 4), ('geb', 3), ('Gordon', 3), ('Banks', 3), ('Univ', 3), ('Computer', 3), ('walrus.mvhs.edu', 3), ('All', 3), ('via', 3), ('believe', 3), ('|', 3), ('too', 3), ('soon', 3), ('Allison', 3), ('nosebleeds', 3), ('25', 3), ('each', 3), ('hear', 3), ('enterprise.bih.harvard.edu', 3), ('erythromycin', 3), ('NNTP-Posting-Host', 3), ('pneumonia', 3), ('Holland', 3), ('raw', 3), ('patient', 3), ('her', 3), ('aio.jsc.nasa.gov', 3), ('S.', 3), ('wrote', 3), ('foods', 3), ('interest', 3), ('enough', 3), ('idea', 3), ('residue', 3), ('system', 3), ('Therefore', 3), ('low', 3), ('go', 3), ('etc', 3), ('little', 3), ('day', 3), ('give', 3), (\"'re\", 3), ('take', 3), ('starting', 3), ('Good', 3), ('real', 3), ('say', 3), ('dyer', 3), ('Dyer', 3), ('remember', 3), ('back', 3), ('new', 3), ('these', 3), ('me', 3), ('different', 3), ('substances', 3), ('then', 3), ('Walter', 3), ('Lundby', 3), ('Motorola', 3), ('TO', 3), ('THAT', 3), ('DO', 3), ('2', 3), ('dye', 3), ('#', 3), ('why', 3), ('tm', 3), ('office', 3), ('pictures', 3), ('find', 3), ('Phil', 3), ('Fraering', 3), ('pgf', 3), ('Caste', 3), ('Article-I.D', 3), ('Station', 3), ('maybe', 3), ('program', 3), ('Spiros', 3), ('Electronics', 3), ('world', 3), ('his', 3), ('reward', 3), ('713', 3), ('Goldin', 3), ('clj', 3), ('ksr.com', 3), ('Chris', 3), ('Jones', 3), ('Proton', 3), ('Mir', 3), ('hsc.usc.edu', 3), ('cs.pitt.edu', 2), ('ktodd', 2), ('rare', 2), ('disease', 2), ('respond', 2), ('email', 2), ('actually', 2), ('SND', 2), ('case', 2), ('therapy', 2), ('much', 2), ('ab961', 2), ('Freenet.carleton.ca', 2), ('18', 2), ('between', 2), ('tried', 2), ('rind', 2), ('Rind', 2), ('School', 2), ('11', 2), ('treating', 2), ('young', 2), ('choice', 2), ('since', 2), ('two', 2), ('pneumoniae', 2), ('Distribution', 2), ('vegetables', 2), ('obstruction', 2), ('dietary', 2), ('restriction', 2), ('otherwise', 2), ('certainly', 2), ('bring', 2), ('enhanced', 2), ('due', 2), ('feeling', 2), ('changed', 2), ('current', 2), ('even', 2), ('lack', 2), ('small', 2), ('major', 2), ('problem', 2), ('digest', 2), ('avoided', 2), ('again', 2), ('items', 2), ('same', 2), ('fruit', 2), ('nuts', 2), ('giving', 2), ('fudge', 2), ('appropriate', 2), ('felt', 2), ('though', 2), ('grain', 2), ('mostly', 2), ('long', 2), ('digestive', 2), ('processing', 2), ('better', 2), ('age', 2), ('whole', 2), ('want', 2), ('possible', 2), ('among', 2), ('how', 2), ('bad', 2), ('As', 2), ('Do', 2), ('given', 2), ('Spencer', 2), ('tell', 2), ('avoid', 2), ('sure', 2), ('second', 2), ('end', 2), ('months', 2), ('issue', 2), ('worth', 2), ('And', 2), ('Luck', 2), ('seizures', 2), ('cup.portal.com', 2), ('interesting', 2), ('anecdotal', 2), ('results', 2), ('sensitive', 2), ('chemicals', 2), ('show', 2), ('Of', 2), ('course', 2), ('children', 2), ('{', 2), ('}', 2), ('et', 2), ('Software', 2), ('They', 2), ('HELP', 2), ('ME', 2), ('sp', 2), ('nutrasweet', 2), ('Division', 2), ('couple', 2), ('Some', 2), ('formaldehyde', 2), ('known', 2), ('methanol', 2), ('uses', 2), ('eliminate', 2), ('whether', 2), ('levels', 2), ('damage', 2), ('toxic', 2), ('cells', 2), ('pregnant', 2), ('easily', 2), ('detected', 2), ('sensitivity', 2), ('Inc.', 2), ('such', 2), ('WHY', 2), ('industry', 2), ('IT', 2), ('GET', 2), ('NOT', 2), ('Why', 2), ('outlaw', 2), ('use', 2), ('large', 2), ('those', 2), ('free', 2), ('yellow', 2), ('junk', 2), ('Bank', 2), ('ecs.comm.mot.com', 2), ('Sector', 2), ('28', 2), ('ob-gyn', 2), ('fetal', 2), ('On', 2), ('ultrasounds', 2), ('something', 2), ('expert', 2), ('Should', 2), ('opinion', 2), ('really', 2), ('ca', 2), ('Any', 2), ('G.', 2), ('srl03.cacs.usl.edu', 2), ('human', 2), ('aurora.alaska.edu', 2), ('Alaska', 2), ('Fairbanks', 2), ('revolt', 2), ('files', 2), ('mean', 2), ('design', 2), ('look', 2), ('==', 2), ('Michael', 2), ('Adams', 2), ('jacked', 2), ('c23st', 2), ('kocrsv01.delcoelect.com', 2), ('Triantafyllopoulos', 2), ('radio', 2), ('commercial', 2), ('Delco', 2), ('This', 2), ('heard', 2), ('begin', 2), ('offices', 2), ('Exploration', 2), ('Sciences', 2), ('position', 2), ('seemed', 2), ('someone', 2), ('interested', 2), ('done', 2), ('old', 2), ('400', 2), ('mission', 2), ('20', 2), ('stuff', 2), ('trying', 2), ('moon', 2), ('developed', 2), ('Jenks', 2), ('NASA/JSC/GM2', 2), ('Shuttle', 2), ('Program', 2), ('483-4368', 2), ('Daniel', 2), ('Administrator', 2), ('Russians', 2), ('resources', 2), ('wonder', 2), ('Ralph', 2), ('29', 2), ('sc', 2), ('Here', 2), ('consider', 2), ('Proton/Centaur', 2), ('prb', 2), ('Pat', 2), ('zeus.calpoly.edu', 2), ('Centaur', 2), ('versions', 2), ('orbits', 2), ('Egan', 2), ('addresses', 2), ('companies', 2), ('Houston', 2), ('TX', 2), ('khayash', 2), ('our', 2), ('rest', 2), ('live', 2), ('technical', 2), ('analysis', 2), ('mccall', 2), ('ETHER', 2), ('speak', 2), ('Striato', 1), ('Nigral', 1), ('Degeneration', 1), ('Pittsburgh', 1), ('Science', 1), ('16', 1), ('9303252134.AA09923', 1), ('Todd', 1), ('information', 1), ('available', 1), ('understand', 1), ('operation', 1), ('referred', 1), ('POLLIDOTOMY', 1), ('order', 1), ('physician', 1), ('performs', 1), ('procedure', 1), ('responses', 1), ('appreciated', 1), ('Please', 1), ('Many', 1), ('cases', 1), (\"Parkinson's\", 1), ('Disease', 1), ('turn', 1), ('autopsy', 1), ('suspected', 1), ('Parkinsonism', 1), ('tremor', 1), ('L-dopa', 1), ('pallidotomy', 1), ('N3JXP', 1), ('Skepticism', 1), ('chastity', 1), ('intellect', 1), ('cadre.dsl.pitt.edu', 1), ('shameful', 1), ('surrender', 1), ('Frequent', 1), ('National', 1), ('Capital', 1), ('Freenet', 1), ('15', 1), ('week', 1), ('result', 1), ('genetic', 1), ('predisposition', 1), ('weak', 1), ('capillary', 1), ('walls', 1), ('Osler-Weber-Rendu', 1), ('Fortunately', 1), ('nosebleed', 1), ('short', 1), ('duration', 1), ('method', 1), ('reduce', 1), ('frequency', 1), ('younger', 1), ('brothers', 1), ('skin', 1), ('transplant', 1), ('thigh', 1), ('nose', 1), ('lining', 1), ('returned', 1), ('seen', 1), ('reference', 1), ('herb', 1), ('Rutin', 1), ('supposed', 1), ('help', 1), ('experiences', 1), ('techniques', 1), ('Ottawa', 1), ('Ontario', 1), ('CANADA', 1), ('Beth', 1), ('Israel', 1), ('Hospital', 1), ('Harvard', 1), ('Medical', 1), ('Boston', 1), ('Mass.', 1), ('USA', 1), ('47974', 1), ('sdcc12.ucsd.edu', 1), ('wsun', 1), ('jeeves.ucsd.edu', 1), ('Fiberman', 1), ('effective', 1), ('depends', 1), ('bacterial', 1), ('otherwise-healthy', 1), ('non-smokers', 1), ('usually', 1), ('considered', 1), ('antibiotic', 1), ('covers', 1), ('most-common', 1), ('pathogens', 1), ('strep', 1), ('mycoplasma', 1), ('uabdpo.dpo.uab.edu', 1), ('gila005', 1), ('Stephen', 1), ('IBD', 1), ('Gastroenterology', 1), ('Alabama', 1), ('usa', 1), ('81', 1), ('Summary', 1), ('thread', 1), ('Crohns', 1), ('unspecified', 1), ('replies', 1), ('mild', 1), ('Avoid', 1), ('plug', 1), ('general', 1), ('1993Apr22.210631.13300', 1), ('spenser', 1), ('fudd.jsc.nasa.gov', 1), ('Spenser', 1), ('Aden', 1), ('Interesting', 1), ('statements', 1), (\"I'm\", 1), ('questioning', 1), ('claims', 1), ('am', 1), ('agree', 1), ('recurrence', 1), ('either', 1), ('mildly', 1), ('DRASTICALLY', 1), ('obout', 1), ('GI', 1), ('community', 1), ('induced', 1), ('evidence', 1), ('deprived', 1), ('mucosal', 1), ('atrophy', 1), ('stimulation', 1), ('intestinal', 1), ('growth', 1), ('factors', 1), ('providing', 1), ('amounts', 1), ('nasogastric', 1), ('feeding', 1), ('IV', 1), ('nutrition', 1), ('digress', 1), ('Symptoms', 1), ('drastically', 1), ('Having', 1), ('resulting', 1), ('resection', 1), ('caveat', 1), ('LOW', 1), ('RESIDUE', 1), ('Basically', 1), ('gut', 1), ('realized', 1), ('caught', 1), ('folds', 1), ('constantly', 1), ('irritate', 1), ('thus', 1), ('exacerbating', 1), ('completely', 1), ('common', 1), ('With', 1), ('typical', 1), ('terminal', 1), ('ileum', 1), ('consisting', 1), ('Completely', 1), ('never', 1), ('corn', 1), ('kernel', 1), ('husk', 1), ('most', 1), ('us', 1), ('popcorn', 1), ('dried', 1), ('dehydrated', 1), ('skins', 1), ('Very', 1), ('tough', 1), ('comes', 1), ('still', 1), ('obstructions', 1), ('Again', 1), ('These', 1), ('stuck', 1), ('ahead', 1), (';', 1), ('Discouraged', 1), ('greatly', 1), ('fibrous', 1), ('wheat', 1), ('breads', 1), ('exotic', 1), ('lettuce', 1), ('iceberg', 1), ('ok', 1), ('apparently', 1), ('water', 1), ('greens', 1), ('turnip', 1), ('mustard', 1), ('kale', 1), ('seeds', 1), ('sesame', 1), ('Arby', 1), ('wild', 1), ('rice', 1), ('husky', 1), ('beans', 1), (\"'ll\", 1), ('generate', 1), ('gas', 1), ('alone', 1), ('BASICALLY', 1), ('requires', 1), ('heavy', 1), ('processed', 1), ('rather', 1), ('ironic', 1), ('PREVENTATIVE', 1), ('your', 1), ('chance', 1), ('inflame', 1), ('NUMEROUS', 1), ('heavily', 1), ('discouraged', 1), ('listed', 1), ('ones', 1), ('wanted', 1), ('Remember', 1), ('remission', 1), ('Veggies', 1), ('cook', 1), ('daylights', 1), ('prefer', 1), ('steaming', 1), ('cooks', 1), ('thoroughly', 1), ('mileage', 1), ('vary', 1), ('else', 1), ('CHECK', 1), ('WITH', 1), ('YOUR', 1), ('DOCTOR', 1), ('word', 1), ('info', 1), ('discussion', 1), ('luck', 1), ('makes', 1), ('especially', 1), ('having', 1), ('observant', 1), ('informed', 1), ('Would', 1), ('many', 1), ('changing', 1), ('Be', 1), ('tested', 1), ('time', 1), (\"Crohn's\", 1), ('commonly', 1), ('intermittent', 1), ('symptoms', 1), ('severly', 1), ('restricted', 1), ('diets', 1), ('renormalize', 1), ('CCFA', 1), ('newsletter', 1), ('recently', 1), ('discussed', 1), ('fiber', 1), ('reading', 1), ('always', 1), ('dealing', 1), ('spdcc.com', 1), ('food-related', 1), ('S.P', 1), ('Consulting', 1), ('Cambridge', 1), ('MA', 1), ('79727', 1), ('mmm', 1), ('Mark', 1), ('Thorson', 1), ('hearing', 1), ('years', 1), ('hyperactivity', 1), ('involved', 1), ('aggressively', 1), ('eliminating', 1), ('artificial', 1), ('coloring', 1), ('flavoring', 1), ('theory', 1), ('backed', 1), ('certain', 1), ('way', 1), ('connection', 1), ('being', 1), ('made', 1), ('hurt', 1), ('all-natural', 1), ('Yeah', 1), ('Feingold', 1), ('Diet', 1), ('load', 1), ('crap', 1), ('Children', 1), ('diagnosed', 1), ('ADD', 1), ('placed', 1), ('improvement', 1), ('intellectual', 1), ('social', 1), ('skills', 1), ('fact', 1), ('continue', 1), ('decline', 1), ('parents', 1), ('enthusiastic', 1), ('approach', 1), ('lap', 1), ('expense', 1), ('development', 1), ('So', 1), ('value', 1), ('People', 1), ('ursa-major.spdcc.com', 1), ('aka', 1), ('ima', 1), ('harvard', 1), ('rayssd', 1), ('linus', 1), ('m2c', 1), ('spdcc', 1), ('Mark-Tarbell', 1), ('suite.com', 1), ('Amniocentesis', 1), ('al', 1), ('Suite', 1), ('7', 1), ('suite', 1), ('tarbell', 1), ('uunet.uu.net', 1), ('gilgamesh.suite.com', 1), ('difference', 1), ('purposes', 1), ('behind', 1), ('amniocentesis', 1), ('chorionic', 1), ('villi', 1), ('sampling', 1), ('sound', 1), ('similar', 1), ('intended', 1), ('detect', 1), ('Thanks', 1), ('\\x03', 1), ('mmatusev', 1), ('radford.vak12ed.edu', 1), ('Melissa', 1), ('N.', 1), ('Matusevich', 1), ('INJECT', 1), ('Virginia', 1), ('Public', 1), ('Education', 1), ('Network', 1), ('Radford', 1), ('According', 1), ('previous', 1), ('poster', 1), ('seek', 1), (\"doctor's\", 1), ('assistance', 1), ('injections', 1), ('Sumatriptin', 1), ('inject', 1), ('oneself', 1), ('immediately', 1), ('upon', 1), ('onset', 1), ('migraine', 1), ('hbloom', 1), ('moose.uvm.edu', 1), ('Heather', 1), ('re', 1), ('aspartame', 1), ('Vermont', 1), ('EMBA', 1), ('Facility', 1), ('21', 1), ('Nutrasweet', 1), ('synthetic', 1), ('sweetener', 1), ('thousand', 1), ('times', 1), ('sweeter', 1), ('sugar', 1), ('produces', 1), ('degrades', 1), ('thought', 1), ('form', 1), ('degredation', 1), ('pathway', 1), ('produced', 1), ('significant', 1), ('both', 1), ('living', 1), ('consume', 1), ('Phenylalanine', 1), ('nothing', 1), ('worry', 1), ('amino', 1), ('acid', 1), ('everyone', 1), ('quantities', 1), ('protein', 1), ('synthesis', 1), ('phenylketoneurea', 1), ('missing', 1), ('enzyme', 1), ('necessary', 1), ('degrade', 1), ('compound', 1), ('accumulate', 1), ('growing', 1), ('nerve', 1), ('Only', 1), ('until', 1), ('10', 1), ('women', 1), ('disorder', 1), ('leading', 1), ('brain', 1), ('infants', 1), ('birth', 1), ('must', 1), ('comsumption', 1), ('phenylalanine', 1), ('child', 1), ('-heather', 1), ('lundby', 1), ('rtsg.mot.com', 1), ('F.', 1), ('superstition', 1), ('accord2', 1), ('Cellular', 1), ('Infrastructure', 1), ('Group', 1), ('48', 1), ('thing', 1), ('monosodium', 1), ('glutamate', 1), ('Superstition', 1), ('Anybody', 1), ('here', 1), ('experience', 1), ('contrary', 1), ('msg', 1), ('whose', 1), ('kids', 1), ('WANT', 1), ('KNOW', 1), ('INDUSTRY', 1), ('WANTS', 1), ('PUT', 1), ('Somebody', 1), ('GIVE', 1), ('REASONS', 1), ('AN', 1)]\n",
            ",\n",
            "[158, 152, 142, 139, 136, 124, 88, 86, 83, 74, 73, 63, 63, 58, 54, 53, 42, 37, 36, 36, 35, 32, 31, 30, 26, 24, 23, 21, 21, 20, 20, 20, 20, 19, 18, 17, 17, 16, 16, 15, 15, 15, 14, 13, 13, 13, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "total number of words: 4304\n"
          ]
        }
      ],
      "source": [
        "# Get the most common words and build index_to_word and word_to_index vectors\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        " \n",
        "print (f\"Using vocabulary size {vocabulary_size}.\" )\n",
        "print (f\"The least frequent word in our vocabulary is '{vocab[-1][0]}' and appeared {vocab[-1][1]} times.\")\n",
        "print(f\"index to word is {index_to_word[5][2]}\")\n",
        "print(vocab[1][1])\n",
        "\n",
        "total = [x[1] for x in vocab]\n",
        "print(vocab)\n",
        "print(vocab[2][0])\n",
        "print(total)\n",
        "total = sum(total)\n",
        "print(f\"total number of words: {total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위 코드는 주어진 단어 빈도 데이터를 사용하여 단어 집합(vocabulary)을 만드는 작업을 수행합니다.\n",
        "\n",
        "- `word_freq.most_common(vocabulary_size-1)`: `word_freq` 객체에 저장된 단어 빈도 데이터 중에서 가장 빈도가 높은 단어부터 `vocabulary_size-1`개를 선택합니다. 여기서 -1은 빈도가 가장 낮은 단어를 위해 하나의 위치를 남겨두는 것을 의미합니다.\n",
        "\n",
        "- `index_to_word = [x[0] for x in vocab]`: 빈도가 높은 순으로 선택된 단어들을 담은 리스트를 만듭니다. 각 요소는 (단어, 빈도) 튜플에서 단어만 추출합니다.\n",
        "\n",
        "- `index_to_word.append(unknown_token)`: 알려지지 않은 단어를 나타내는 특별한 토큰인 `unknown_token`을 단어 집합에 추가합니다.\n",
        "\n",
        "- `word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])`: 단어와 해당 인덱스를 매핑하는 딕셔너리인 `word_to_index`를 생성합니다. `enumerate(index_to_word)`를 사용하여 각 단어와 해당 인덱스를 순회하고, 이를 딕셔너리로 변환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CxBYzBs1o5N"
      },
      "source": [
        "### Exercise 2.1\n",
        "\n",
        "Code your own TF-IDF representation function and use it on this dataset. (Don't use code from libraries. Build your own function with Numpy/Pandas). Use the formular TFIDF = TF * (IDF+1). The effect of adding “1” to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. The term frequency is the raw count of a term in a document. The inverse document frequency is the natural logarithm of the inverse fraction of the documents that contain the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LAQX0zw11o5N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is this implementation correct?\n",
            "Answer: No\n",
            "    02  041300  07  0815  10  101  10511  11  115397  12  ...  yellow  \\\n",
            "0    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "1    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "2    0       0   0     0   0    0      0   1       0   0  ...       0   \n",
            "3    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "4    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "5    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "6    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "7    0       0   0     0   1    0      0   0       0   0  ...       0   \n",
            "8    0       0   0     0   0    0      0   0       0   0  ...       3   \n",
            "9    0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "10   0       0   0     0   0    0      0   0       0   1  ...       0   \n",
            "11   0       0   0     0   0    0      0   1       0   0  ...       0   \n",
            "12   0       0   0     1   0    0      0   0       0   0  ...       0   \n",
            "13   0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "14   0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "15   1       0   1     0   0    0      0   0       0   0  ...       0   \n",
            "16   0       0   0     0   0    0      0   0       0   0  ...       0   \n",
            "17   0       1   0     0   0    1      0   0       0   0  ...       0   \n",
            "18   0       0   0     0   0    0      1   0       0   0  ...       0   \n",
            "19   0       0   0     0   0    0      0   0       1   0  ...       0   \n",
            "\n",
            "    yeltsin  yet  you  young  younger  your  z3  zeta  zeus  \n",
            "0         0    0    0      0        0     0   0     0     0  \n",
            "1         0    0    0      0        1     0   0     0     0  \n",
            "2         0    0    0      1        0     0   0     0     0  \n",
            "3         0    0    5      0        0     2   0     0     0  \n",
            "4         0    0    0      0        0     0   0     0     0  \n",
            "5         0    0    0      0        0     0   0     0     0  \n",
            "6         0    0    0      0        0     0   0     0     0  \n",
            "7         0    0    1      1        0     0   0     0     0  \n",
            "8         0    0    1      0        0     0   0     0     0  \n",
            "9         0    0    0      0        0     0   0     0     0  \n",
            "10        0    0    0      0        0     0   0     0     0  \n",
            "11        0    0    2      0        0     0   0     0     0  \n",
            "12        0    0    0      0        0     0   0     0     0  \n",
            "13        1    0    2      0        0     0   0     0     0  \n",
            "14        0    0    2      0        0     0   0     0     0  \n",
            "15        0    0    0      0        0     0   1     1     0  \n",
            "16        0    0    0      0        0     0   0     0     1  \n",
            "17        0    0    2      0        0     0   0     0     0  \n",
            "18        0    0    1      0        0     0   0     0     0  \n",
            "19        0    1    1      0        0     0   0     0     1  \n",
            "\n",
            "[20 rows x 1440 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvec = CountVectorizer()\n",
        "df = pd.DataFrame(countvec.fit_transform(data).toarray(), columns=countvec.get_feature_names_out())\n",
        "\n",
        "def tfidf(df):\n",
        "    # TFIDF = TF * (IDF + 1)\n",
        "    # idf에 1을 더하는 이유는 모든 단어가 적어도 한 번은 나타나야 하기 때문. idf가 0이 되는 것을 방지.\n",
        "    # TF(d, w) = (특정 단어 w의 문서 d 내 등장 횟수) / (해당 문서 d 내 총 단어 수)\n",
        "    # IDF(w) = log_e(총 문서 수 / w를 포함한 문서 수)\n",
        "    pass\n",
        "    '''\n",
        "    ## tokenizing words\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in df]\n",
        "    print(tokenized_sentences)\n",
        "    #unknown_token = 'unknown'\n",
        "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences)) # word frequency in tokenized sentences\n",
        "    print (f\"Found {len(word_freq.items())} unique words tokens.\")\n",
        "    \n",
        "    vocabulary_size = len(word_freq.items())\n",
        "    vocab = word_freq.most_common(vocabulary_size-1)\n",
        "    \n",
        "    # calculate term frequency ---\n",
        "    ## total number of words\n",
        "    #total = sum(word_freq.values()) # total number of words in all documents\n",
        "    \n",
        "    total = [x[1] for x in vocab]\n",
        "    total = sum(total)\n",
        "    print(f\"total number of words: {total}\")\n",
        "    \n",
        "    \n",
        "    word_tf = [x[0] for x in vocab]\n",
        "    freq_tf = [x[1]/total for x in vocab]# term frequency\n",
        "    tf = dict(zip(word_tf, freq_tf))\n",
        "    \n",
        "    # calculate inverse document frequency ---\n",
        "    ## 문서 내에 단어가 있는지 여부를 나타내는 딕셔너리 생성\n",
        "    ## create dictionary to indicate whether word is in document\n",
        "    \n",
        "    document_contains_word = {}\n",
        "     for column in df.columns:\n",
        "      for doc in df[column]:  # 'column_name'을 실제 텍스트가 있는 열 이름으로 바꿔주세요.\n",
        "        for word in set(nltk.word_tokenize(doc)):   # 문서를 단어로 분할하고 중복 제거\n",
        "          if word in document_contains_word:  # 만약 단어가 이미 딕셔너리에 있으면\n",
        "            document_contains_word[word] += 1 # 해당 단어가 포함된 문서 수를 1 증가\n",
        "          else:                 # 없으면\n",
        "            document_contains_word[word] = 1  # 새로운 단어로 추가\n",
        "\n",
        "    \n",
        "    # IDF 계산\n",
        "    idf = {}\n",
        "    num_documents = len(df)\n",
        "    for word, doc_count in document_contains_word.items():\n",
        "        idf[word] = np.log(num_documents / doc_count)        # 전체 문서 수\n",
        "        \n",
        "    tfidf = {}\n",
        "    for doc_index, doc in enumerate(df):\n",
        "        tfidf[doc_index] = {}\n",
        "        for word in doc:\n",
        "            tfidf[doc_index][word] = tf[word] * (idf[word] + 1)  # TF * (IDF + 1)\n",
        "    \n",
        "    return tfidf\n",
        "    '''\n",
        "    # at least i tried\n",
        "    \n",
        "    return None\n",
        "\n",
        "tfidf(df)\n",
        "    \n",
        "    \n",
        "rep = tfidf(df)\n",
        "\n",
        "# Check if your implementation is correct\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(norm=None, smooth_idf=False, use_idf=True)\n",
        "X_train = pd.DataFrame(vectorizer.fit_transform(data).toarray(), columns=countvec.get_feature_names_out())\n",
        "answer=['No','Yes']\n",
        "epsilon = 0.0001\n",
        "if rep is None: \n",
        "  print (f'Is this implementation correct?\\nAnswer: {answer[0]}')\n",
        "if rep is not None:\n",
        "  print (f'Is this implementation correct?\\nAnswer: {answer[1*np.all((X_train - rep) < epsilon)]}')\n",
        "  \n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yvgshAez8XY2"
      },
      "outputs": [
        {
          "ename": "InvalidParameterError",
          "evalue": "The 'X' parameter of cosine_similarity must be an array-like or a sparse matrix. Got None instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# an example of what to do with these similarities:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# analysis with tf-idf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[1;32m----> 7\u001b[0m similiarities \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# measure of the similarity of the direction of two vectors\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
            "\u001b[1;31mInvalidParameterError\u001b[0m: The 'X' parameter of cosine_similarity must be an array-like or a sparse matrix. Got None instead."
          ]
        }
      ],
      "source": [
        "# an example of what to do with these similarities:\n",
        "\n",
        "\n",
        "# analysis with tf-idf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similiarities = cosine_similarity(rep, rep) # measure of the similarity of the direction of two vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39RjLr9J8b8f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'similiarities' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[158], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mfill_diagonal(\u001b[43msimiliarities\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m max_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munravel_index(similiarities\u001b[38;5;241m.\u001b[39margmax(), similiarities\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m similiarities[max_ind] \u001b[38;5;66;03m# highest similarity of two documents\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'similiarities' is not defined"
          ]
        }
      ],
      "source": [
        "np.fill_diagonal(similiarities, 0)\n",
        "max_ind = np.unravel_index(similiarities.argmax(), similiarities.shape)\n",
        "similiarities[max_ind] # highest similarity of two documents"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Problem_Analysis_and_Data_Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
